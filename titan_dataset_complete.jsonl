{"instruction": "Implement a Domain-Driven Design (DDD) Aggregate Root for a Banking Ledger using Event Sourcing. Ensure strict immutability and version control to prevent race conditions.", "output": "from __future__ import annotations\nfrom dataclasses import dataclass, field\nfrom typing import List, Protocol, TypeVar, Generic, Sequence\nfrom uuid import UUID\nfrom datetime import datetime, timezone\n\n# --- CORE EVENTS (Immutable Facts) ---\n\n@dataclass(frozen=True)\nclass DomainEvent:\n    event_id: UUID\n    occurred_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n\n@dataclass(frozen=True)\nclass AccountOpened(DomainEvent):\n    account_id: UUID\n    owner: str\n    initial_balance: int\n\n@dataclass(frozen=True)\nclass FundsDeposited(DomainEvent):\n    amount: int\n    new_balance: int\n\n# --- AGGREGATE ROOT (The State Machine) ---\n\nclass AggregateRoot:\n    def __init__(self, id: UUID, version: int = 0):\n        self.id = id\n        self._version = version\n        self._changes: List[DomainEvent] = []\n\n    def get_uncommitted_changes(self) -> Sequence[DomainEvent]:\n        return tuple(self._changes)\n\n    def mark_changes_as_committed(self) -> None:\n        self._changes.clear()\n\n    def _apply_change(self, event: DomainEvent, is_new: bool = True) -> None:\n        self.apply(event)\n        if is_new:\n            self._changes.append(event)\n\n    def apply(self, event: DomainEvent) -> None:\n        raise NotImplementedError\n\n# --- BANK ACCOUNT IMPLEMENTATION ---\n\nclass BankAccount(AggregateRoot):\n    def __init__(self, id: UUID):\n        super().__init__(id)\n        self.balance: int = 0\n        self.is_active: bool = False\n\n    @classmethod\n    def open(cls, id: UUID, owner: str, opening_balance: int) -> BankAccount:\n        account = cls(id)\n        if opening_balance < 0:\n            raise ValueError('Cannot open account with negative funds')\n        \n        account._apply_change(AccountOpened(UUID('...'), id, owner, opening_balance))\n        return account\n\n    def deposit(self, amount: int) -> None:\n        if not self.is_active:\n            raise RuntimeError('Account is closed')\n        if amount <= 0:\n            raise ValueError('Deposit must be positive')\n        \n        new_balance = self.balance + amount\n        self._apply_change(FundsDeposited(UUID('...'), amount, new_balance))\n\n    def apply(self, event: DomainEvent) -> None:\n        # The Internal Policy Logic\n        if isinstance(event, AccountOpened):\n            self.is_active = True\n            self.balance = event.initial_balance\n        elif isinstance(event, FundsDeposited):\n            self.balance = event.new_balance"}
{"instruction": "Design a 'Circuit Breaker' pattern decorator in Python to prevent cascading failures in a distributed system. Include states for OPEN, CLOSED, and HALF-OPEN.", "output": "import time\nimport functools\nfrom enum import Enum\nfrom typing import Callable, Any, Type\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass State(Enum):\n    CLOSED = 'CLOSED'      # Normal operation\n    OPEN = 'OPEN'          # Failing, block requests\n    HALF_OPEN = 'HALF_OPEN' # Testing if service recovered\n\nclass CircuitOpenError(Exception):\n    pass\n\n@dataclass\nclass CircuitConfig:\n    failure_threshold: int = 3\n    recovery_timeout: float = 30.0  # seconds\n    expected_exceptions: tuple[Type[Exception], ...] = (ConnectionError, TimeoutError)\n\nclass CircuitBreaker:\n    def __init__(self, config: CircuitConfig):\n        self.config = config\n        self.state = State.CLOSED\n        self.failure_count = 0\n        self.last_failure_time = 0.0\n\n    def __call__(self, func: Callable[..., Any]) -> Callable[..., Any]:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            self._check_state()\n            \n            try:\n                result = func(*args, **kwargs)\n                self._on_success()\n                return result\n            except self.config.expected_exceptions as e:\n                self._on_failure()\n                logger.error(f'Circuit caught error: {e}')\n                raise\n        return wrapper\n\n    def _check_state(self):\n        if self.state == State.OPEN:\n            elapsed = time.time() - self.last_failure_time\n            if elapsed > self.config.recovery_timeout:\n                logger.info('Circuit entering HALF-OPEN state')\n                self.state = State.HALF_OPEN\n            else:\n                raise CircuitOpenError(f'Circuit is OPEN. Retry in {self.config.recovery_timeout - elapsed:.1f}s')\n\n    def _on_success(self):\n        if self.state == State.HALF_OPEN:\n            logger.info('Circuit recovering to CLOSED')\n            self.state = State.CLOSED\n            self.failure_count = 0\n\n    def _on_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        if self.state == State.HALF_OPEN or self.failure_count >= self.config.failure_threshold:\n            self.state = State.OPEN\n            logger.critical('Circuit tripped to OPEN')"}
{"instruction": "Create a 'Specification Pattern' implementation for complex business rule validation. Use composite patterns (AND, OR, NOT) to combine rules.", "output": "from abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar, Any\n\nT = TypeVar('T')\n\nclass Specification(ABC, Generic[T]):\n    @abstractmethod\n    def is_satisfied_by(self, candidate: T) -> bool:\n        raise NotImplementedError\n\n    def __and__(self, other: Specification[T]) -> Specification[T]:\n        return AndSpecification(self, other)\n\n    def __or__(self, other: Specification[T]) -> Specification[T]:\n        return OrSpecification(self, other)\n\n    def __invert__(self) -> Specification[T]:\n        return NotSpecification(self)\n\nclass AndSpecification(Specification[T]):\n    def __init__(self, left: Specification[T], right: Specification[T]):\n        self.left = left\n        self.right = right\n    def is_satisfied_by(self, candidate: T) -> bool:\n        return self.left.is_satisfied_by(candidate) and self.right.is_satisfied_by(candidate)\n\nclass OrSpecification(Specification[T]):\n    def __init__(self, left: Specification[T], right: Specification[T]):\n        self.left = left\n        self.right = right\n    def is_satisfied_by(self, candidate: T) -> bool:\n        return self.left.is_satisfied_by(candidate) or self.right.is_satisfied_by(candidate)\n\nclass NotSpecification(Specification[T]):\n    def __init__(self, wrapped: Specification[T]):\n        self.wrapped = wrapped\n    def is_satisfied_by(self, candidate: T) -> bool:\n        return not self.wrapped.is_satisfied_by(candidate)\n\n# --- USAGE EXAMPLE (S+ Context) ---\n# Ideally, training data should show HOW to use the pattern.\n\n@dataclass\nclass User:\n    age: int\n    is_premium: bool\n    has_debt: bool\n\nclass IsAdult(Specification[User]):\n    def is_satisfied_by(self, candidate: User) -> bool:\n        return candidate.age >= 18\n\nclass IsPremium(Specification[User]):\n    def is_satisfied_by(self, candidate: User) -> bool:\n        return candidate.is_premium\n\n# Composite Logic:\n# rule = IsAdult() & (IsPremium() | ~HasDebt())"}
{"instruction": "Implement a Thread-Safe Singleton using the 'Borg' (Monostate) pattern variation for shared configuration management.", "output": "from typing import Any, Dict\nimport threading\n\nclass ConfigurationBorg:\n    \"\"\"\n    The Borg pattern: Instances are different, but they share the same state.\n    This is often preferred over the strict Singleton pattern in Python.\n    Includes thread-safety for concurrent access.\n    \"\"\"\n    _shared_state: Dict[str, Any] = {}\n    _lock = threading.Lock()\n\n    def __init__(self):\n        self.__dict__ = self._shared_state\n\n    def set_config(self, key: str, value: Any) -> None:\n        with self._lock:\n            self._shared_state[key] = value\n\n    def get_config(self, key: str, default: Any = None) -> Any:\n        # Reads are atomic in Python dicts, but lock ensures consistency if logic expands\n        with self._lock:\n            return self._shared_state.get(key, default)\n\n    def __str__(self) -> str:\n        return str(self._shared_state)"}
{"instruction": "Write a Unit of Work (UoW) Context Manager using Abstract Base Classes to manage database transactions and decouple the service layer from the database.", "output": "from __future__ import annotations\nfrom types import TracebackType\nfrom typing import Optional, Type\nfrom abc import ABC, abstractmethod\n\nclass AbstractRepository(ABC):\n    @abstractmethod\n    def add(self, entity: Any): ...\n    @abstractmethod\n    def get(self, id: Any): ...\n\nclass AbstractUnitOfWork(ABC):\n    \"\"\"\n    Atomic Unit of Work.\n    Ensures that either all changes are committed or none are (Atomicity).\n    \"\"\"\n    users: AbstractRepository  # Example repository\n\n    def __enter__(self) -> AbstractUnitOfWork:\n        return self\n\n    def __exit__(\n        self, \n        exc_type: Optional[Type[BaseException]], \n        exc_val: Optional[BaseException], \n        exc_tb: Optional[TracebackType]\n    ) -> None:\n        if exc_type:\n            self.rollback()\n        else:\n            self.commit()\n\n    @abstractmethod\n    def commit(self) -> None:\n        raise NotImplementedError\n\n    @abstractmethod\n    def rollback(self) -> None:\n        raise NotImplementedError\n\n# --- CONCRETE IMPLEMENTATION (SQLAlchemy Style) ---\n\nclass SqlAlchemyUnitOfWork(AbstractUnitOfWork):\n    def __init__(self, session_factory):\n        self.session_factory = session_factory\n\n    def __enter__(self):\n        self.session = self.session_factory()\n        self.users = SqlAlchemyRepository(self.session)\n        return super().__enter__()\n\n    def __exit__(self, *args):\n        super().__exit__(*args)\n        self.session.close()\n\n    def commit(self):\n        self.session.commit()\n\n    def rollback(self):\n        self.session.rollback()"}
{"instruction": "Design an Asynchronous Data Pipeline using Python `asyncio` queues, handling backpressure and graceful shutdowns.", "output": "import asyncio\nimport logging\nfrom dataclasses import dataclass\nfrom typing import AsyncGenerator\n\nlogger = logging.getLogger('Pipeline')\n\n@dataclass\nclass Packet:\n    id: int\n    payload: str\n\nasync def producer(queue: asyncio.Queue, count: int):\n    \"\"\"Generates data and respects queue limits (Backpressure).\"\"\"\n    for i in range(count):\n        item = Packet(id=i, payload=f'Data-{i}')\n        await queue.put(item)  # Blocks if queue is full\n        logger.info(f'Produced: {item.id}')\n    \n    # Sentinel value to signal completion\n    await queue.put(None)\n\nasync def consumer(queue: asyncio.Queue, worker_id: int):\n    \"\"\"Consumes data until sentinel is received.\"\"\"\n    while True:\n        item = await queue.get()\n        if item is None:\n            # Put it back for other workers to see\n            await queue.put(None)\n            queue.task_done()\n            break\n        \n        try:\n            logger.info(f'Worker {worker_id} processing {item.id}...')\n            await asyncio.sleep(0.1)  # Simulate IO work\n        except Exception as e:\n            logger.error(f'Worker {worker_id} failed on {item.id}: {e}')\n        finally:\n            queue.task_done()\n\nasync def main():\n    # Queue size = 5 enforces backpressure (Producer waits if Consumers are slow)\n    pipeline = asyncio.Queue(maxsize=5)\n    \n    # Launch Workers\n    producers = [asyncio.create_task(producer(pipeline, 20))]\n    consumers = [asyncio.create_task(consumer(pipeline, i)) for i in range(3)]\n    \n    await asyncio.gather(*producers)\n    await pipeline.join()  # Wait for queue to drain\n    \n    # Cancel consumers (they are stuck on sentinel loop)\n    for c in consumers:\n        c.cancel()\n    \n    logger.info('Pipeline finished.')"}
{"instruction": "Implement a secure password hashing mechanism using Python's `secrets` module and `hashlib`. Ensure salt uniqueness and constant-time comparison to prevent timing attacks.", "output": "import hashlib\nimport secrets\nimport hmac\nfrom typing import Tuple\n\n# --- CONSTANTS (S+ Density: Secure Defaults) ---\nITERATIONS = 600_000  # OWASP Recommendation for SHA-256\nALGORITHM = 'sha256'\nSALT_SIZE = 32\n\nclass PasswordManager:\n    @staticmethod\n    def hash_password(password: str) -> str:\n        \"\"\"Hashes password using PBKDF2-HMAC-SHA256 with a random salt.\"\"\"\n        # 1. Generate CSPRNG Salt (Cryptographically Secure Pseudo-Random Number Generator)\n        salt_bytes = secrets.token_bytes(SALT_SIZE)\n        salt_hex = salt_bytes.hex()\n        \n        # 2. Key Derivation\n        dk = hashlib.pbkdf2_hmac(\n            ALGORITHM,\n            password.encode('utf-8'),\n            salt_bytes,\n            ITERATIONS\n        )\n        \n        # 3. Storage Format: alg$iter$salt$hash\n        return f'{ALGORITHM}${ITERATIONS}${salt_hex}${dk.hex()}'\n\n    @staticmethod\n    def verify_password(stored: str, provided: str) -> bool:\n        try:\n            alg, iter_str, salt_hex, hash_hex = stored.split('$')\n            iterations = int(iter_str)\n            salt_bytes = bytes.fromhex(salt_hex)\n            stored_hash = bytes.fromhex(hash_hex)\n        except ValueError:\n            return False  # Malformed hash\n\n        # 4. Re-compute Hash\n        computed_hash = hashlib.pbkdf2_hmac(\n            alg,\n            provided.encode('utf-8'),\n            salt_bytes,\n            iterations\n        )\n\n        # 5. Constant-Time Comparison (Prevent Timing Attacks)\n        # Never use '==' for secrets! It leaks length information.\n        return hmac.compare_digest(stored_hash, computed_hash)"}
{"instruction": "Create a secure Token Bucket Rate Limiter using Redis (simulated) and Lua scripting logic to prevent Race Conditions (TOCTOU) in API endpoints.", "output": "import time\nfrom dataclasses import dataclass\nfrom typing import Dict\n\n# --- REDIS SIMULATION (The Concept is Atomic Operations) ---\n\n@dataclass\nclass Bucket:\n    tokens: float\n    last_updated: float\n\nclass RateLimiter:\n    def __init__(self, capacity: int, refill_rate: float):\n        self.capacity = capacity\n        self.refill_rate = refill_rate  # tokens per second\n        self._store: Dict[str, Bucket] = {}\n\n    def allow_request(self, user_id: str) -> bool:\n        \"\"\"\n        Implements the Token Bucket algorithm.\n        In production, this logic must run inside a Redis LUA script\n        to guarantee atomicity across distributed nodes.\n        \"\"\"\n        now = time.time()\n        bucket = self._store.get(user_id)\n\n        if not bucket:\n            bucket = Bucket(tokens=self.capacity, last_updated=now)\n            self._store[user_id] = bucket\n        \n        # 1. Refill Tokens (Lazy Refill)\n        elapsed = now - bucket.last_updated\n        added = elapsed * self.refill_rate\n        bucket.tokens = min(self.capacity, bucket.tokens + added)\n        bucket.last_updated = now\n\n        # 2. Consume Token\n        if bucket.tokens >= 1:\n            bucket.tokens -= 1\n            return True\n        else:\n            return False"}
{"instruction": "Design a Python class that handles AES-GCM (Galois/Counter Mode) encryption. Enforce the use of a unique Nonce for every encryption operation.", "output": "from cryptography.hazmat.primitives.ciphers.aead import AESGCM\nimport os\nfrom base64 import b64encode, b64decode\nimport json\n\nclass SecureVault:\n    def __init__(self, key: bytes):\n        \"\"\"AES-GCM requires a 256-bit (32 byte) key.\"\"\"\n        if len(key) != 32:\n            raise ValueError('Key must be exactly 32 bytes')\n        self.aesgcm = AESGCM(key)\n\n    def encrypt(self, plaintext: str, associated_data: str = '') -> str:\n        # 1. Generate Nonce (Number used ONCE)\n        # NIST recommends 12 bytes for GCM. NEVER reuse a nonce with the same key.\n        nonce = os.urandom(12)\n        \n        data = plaintext.encode('utf-8')\n        ad = associated_data.encode('utf-8')\n\n        # 2. Encrypt & Tag (GCM handles Integrity automatically)\n        ciphertext = self.aesgcm.encrypt(nonce, data, ad)\n\n        # 3. Pack result (Nonce + Ciphertext)\n        # We must store the nonce to decrypt, but it's public (not secret).\n        payload = {\n            'n': b64encode(nonce).decode('utf-8'),\n            'c': b64encode(ciphertext).decode('utf-8'),\n            'ad': associated_data\n        }\n        return json.dumps(payload)\n\n    def decrypt(self, packaged_data: str) -> str:\n        try:\n            payload = json.loads(packaged_data)\n            nonce = b64decode(payload['n'])\n            ciphertext = b64decode(payload['c'])\n            ad = payload.get('ad', '').encode('utf-8')\n            \n            # 4. Authenticated Decryption\n            # If the data was tampered with, this raises InvalidTag.\n            plaintext_bytes = self.aesgcm.decrypt(nonce, ciphertext, ad)\n            return plaintext_bytes.decode('utf-8')\n        except Exception as e:\n            raise ValueError('Decryption Failed: Integrity Check Failed') from e"}
{"instruction": "Implement a secure input sanitizer for a Web Controller class. Block XSS (Cross-Site Scripting) by escaping HTML entities and enforce strict Allow-Lists.", "output": "import html\nimport re\nfrom typing import Dict, Any\n\nclass InputSanitizer:\n    # S+ Density: Regex for strict alphanumeric allow-list\n    USERNAME_PATTERN = re.compile(r'^[a-zA-Z0-9_]{3,20}$')\n\n    @staticmethod\n    def sanitize_text(user_input: str) -> str:\n        \"\"\"Prevents Reflected XSS by escaping HTML control characters.\"\"\"\n        if not user_input:\n            return ''\n        # Converts <script> to &lt;script&gt;\n        return html.escape(user_input, quote=True)\n\n    @classmethod\n    def validate_username(cls, username: str) -> str:\n        \"\"\"Enforces strict Allow-List (White-listing).\"\"\"\n        if not cls.USERNAME_PATTERN.match(username):\n            raise ValueError('Invalid Username: Alphanumeric only, 3-20 chars.')\n        return username\n\n    @staticmethod\n    def clean_payload(payload: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Recursively cleans a JSON payload.\"\"\"\n        cleaned = {}\n        for k, v in payload.items():\n            # Sanitize keys to prevent Prototype Pollution\n            safe_key = html.escape(k)\n            \n            if isinstance(v, str):\n                cleaned[safe_key] = html.escape(v)\n            elif isinstance(v, int) or isinstance(v, float):\n                cleaned[safe_key] = v\n            else:\n                # Reject complex nested objects if not expected\n                raise ValueError(f'Type {type(v)} not allowed in payload')\n        return cleaned"}
{"instruction": "Write a Python implementation of the Double-Checked Locking pattern for a Singleton, but explain why it is tricky in Python due to the GIL, and provide a thread-safe alternative.", "output": "import threading\nfrom typing import Optional\n\nclass DatabaseConnection:\n    _instance: Optional['DatabaseConnection'] = None\n    _lock = threading.Lock()\n\n    def __new__(cls):\n        # 1. First Check (Fast, no lock)\n        if cls._instance is None:\n            # 2. Acquire Lock\n            with cls._lock:\n                # 3. Double Check (Safety inside lock)\n                # Another thread might have created it while we waited for lock.\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n                    cls._instance._initialize()\n        return cls._instance\n\n    def _initialize(self):\n        \"\"\"Simulate heavy connection logic.\"\"\"\n        print('Connecting to DB...')\n\n# --- S+ CONTEXT ---\n# In Java/C++, Double-Checked Locking is complex due to memory visibility.\n# In Python, the GIL (Global Interpreter Lock) actually makes atomic operations safer,\n# but the explicit Lock is still required to prevent logical race conditions \n# during the instantiation phase."}
{"instruction": "Implement a Secure Session Manager that rotates Session IDs on privilege change (Login) to prevent Session Fixation attacks.", "output": "import uuid\nfrom typing import Dict, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass Session:\n    id: str\n    user_id: Optional[str] = None\n    is_authenticated: bool = False\n\nclass SessionStore:\n    def __init__(self):\n        self._sessions: Dict[str, Session] = {}\n\n    def create_anonymous_session(self) -> str:\n        sid = str(uuid.uuid4())\n        self._sessions[sid] = Session(id=sid)\n        return sid\n\n    def login(self, current_sid: str, user_id: str) -> str:\n        \"\"\"\n        Elevates privilege.\n        CRITICAL: Must issue a NEW Session ID to prevent Session Fixation.\n        \"\"\"\n        if current_sid not in self._sessions:\n            raise ValueError('Invalid Session')\n\n        # 1. Destroy old session (The Fixation vector)\n        del self._sessions[current_sid]\n\n        # 2. Create new session with elevated privs\n        new_sid = str(uuid.uuid4())\n        new_session = Session(id=new_sid, user_id=user_id, is_authenticated=True)\n        self._sessions[new_sid] = new_session\n        \n        return new_sid  # Send this back to client as new Cookie"}
{"instruction": "Write a Property-Based Test using the `hypothesis` library to verify a sorting algorithm handles edge cases (empty lists, negative numbers, massive integers) automatically.", "output": "from hypothesis import given, strategies as st, settings, Verbosity\nimport unittest\nfrom typing import List\n\n# --- S-CLASS CONTEXT: PROPERTY-BASED TESTING ---\n# Instead of writing 10 manual tests, we describe the 'properties' of the data\n# and let the engine generate 100s of edge cases (fuzzing) to break the code.\n\ndef custom_sort(data: List[int]) -> List[int]:\n    return sorted(data)\n\nclass TestSortingProperties(unittest.TestCase):\n    @settings(max_examples=500, verbosity=Verbosity.quiet)\n    @given(st.lists(st.integers(), min_size=0, max_size=1000))\n    def test_sort_maintains_length_and_content(self, original_list: List[int]):\n        \"\"\"Invariant: Sorted list must have same length and elements as original.\"\"\"\n        result = custom_sort(original_list)\n        \n        # Property 1: Length Conservation\n        self.assertEqual(len(result), len(original_list))\n        \n        # Property 2: Idempotency (Sorting twice shouldn't change result)\n        self.assertEqual(custom_sort(result), result)\n        \n        # Property 3: Ordering\n        # checks if every element is <= the next element\n        self.assertTrue(all(result[i] <= result[i+1] for i in range(len(result)-1)))"}
{"instruction": "Implement a Mocking strategy using `unittest.mock` to test a function that depends on an unstable 3rd party API. Ensure the test handles network timeouts and 500 errors gracefully.", "output": "import unittest\nfrom unittest.mock import patch, Mock\nimport requests\nfrom my_module import fetch_user_data, ApiError # assuming these exist\n\nclass TestApiResilience(unittest.TestCase):\n    @patch('my_module.requests.get')\n    def test_fetch_user_retries_on_500_error(self, mock_get):\n        \"\"\"\n        S-Class Pattern: Deterministic Simulation of Non-Deterministic failures.\n        We program the mock to fail twice with 500, then succeed.\n        \"\"\"\n        # Scenario: Fail (500), Fail (500), Success (200)\n        mock_response_fail = Mock()\n        mock_response_fail.status_code = 500\n        \n        mock_response_ok = Mock()\n        mock_response_ok.status_code = 200\n        mock_response_ok.json.return_value = {'id': 1, 'name': 'Titan'}\n        \n        # side_effect iterates through the list for each call\n        mock_get.side_effect = [mock_response_fail, mock_response_fail, mock_response_ok]\n        \n        # Execution\n        result = fetch_user_data(user_id=1, max_retries=3)\n        \n        # Assertion: Verify the retry logic actually ran 3 times\n        self.assertEqual(mock_get.call_count, 3)\n        self.assertEqual(result['name'], 'Titan')"}
{"instruction": "Create a Pytest fixture for a database transaction that rolls back changes after every test (Clean State pattern) to prevent test pollution.", "output": "import pytest\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom my_app.models import Base\n\n# S-CLASS: TEST ISOLATION\n# Tests must never affect each other. We wrap every test in a transaction\n# and ROLLBACK at the end, so the DB is always empty for the next test.\n\n@pytest.fixture(scope='session')\ndef engine():\n    return create_engine('sqlite:///:memory:')\n\n@pytest.fixture(scope='session')\ndef tables(engine):\n    Base.metadata.create_all(engine)\n    yield\n    Base.metadata.drop_all(engine)\n\n@pytest.fixture\ndef db_session(engine, tables):\n    connection = engine.connect()\n    transaction = connection.begin()\n    Session = sessionmaker(bind=connection)\n    session = Session()\n    \n    yield session\n    \n    # TEARDOWN: Rollback everything the test did\n    session.close()\n    transaction.rollback()\n    connection.close()"}
{"instruction": "Write a Parameterized Test in Pytest to verify a data validation function against a matrix of malicious inputs (SQL Injection, XSS payloads).", "output": "import pytest\nfrom my_security import validate_input, SecurityException\n\n# S-CLASS: DATA-DRIVEN TESTING\n# Separating test logic from test data allows us to scale coverage instantly.\n\nMALICIOUS_PAYLOADS = [\n    (\"' OR '1'='1\", \"SQL Injection Detected\"),\n    (\"<script>alert(1)</script>\", \"XSS Detected\"),\n    (\"../../etc/passwd\", \"Path Traversal Detected\"),\n    (\"A\" * 10000, \"Buffer Overflow Attempt\"),\n    (\"\\x00\\x00\", \"Null Byte Injection\")\n]\n\n@pytest.mark.parametrize(\"payload, expected_error_msg\", MALICIOUS_PAYLOADS)\ndef test_validator_blocks_attacks(payload, expected_error_msg):\n    with pytest.raises(SecurityException) as excinfo:\n        validate_input(payload)\n    \n    assert expected_error_msg in str(excinfo.value)"}
{"instruction": "Implement 'Mutation Testing' logic conceptually: Explain how to verify the quality of the tests themselves by deliberately injecting bugs into the code.", "output": "import ast\nimport subprocess\n\n# --- S-CLASS CONCEPT: MUTATION TESTING ---\n# Standard tests check if the Code is good.\n# Mutation tests check if the TESTS are good.\n# Strategy: We flip a '>' to '<' in the source code. If the test suite still PASSES, \n# then the tests are weak (they failed to catch the mutation).\n\n\"\"\"\nScenario:\nOriginal Code: if x > 10: return True\nMutation:      if x < 10: return True\n\nAction:\n1. Parse source code into AST (Abstract Syntax Tree).\n2. Traverse AST and apply mutation operators (Bitwise inversion, operator swapping).\n3. Run 'pytest'.\n4. If pytest exits with CODE 0 (Success), the Mutant Survived -> TEST SUITE FAILED.\n5. If pytest exits with CODE 1 (Failure), the Mutant was Killed -> TEST SUITE PASSED.\n\"\"\"\n# Tools like 'mutmut' or 'Cosmic Ray' automate this in Python."}
{"instruction": "Design a load test using `locust` to simulate concurrent user behavior and analyze system latency under stress (P99 latency).", "output": "from locust import HttpUser, task, between, constant_pacing\n\n# S-CLASS: PERFORMANCE PROFILING\n# We care about the 99th Percentile (P99), not the Average.\n# The Average hides the fact that 1% of users are waiting 10 seconds.\n\nclass WebsiteUser(HttpUser):\n    # Strict pacing: Task runs exactly once every second per user\n    wait_time = constant_pacing(1)\n\n    @task(3)  # Higher weight = more frequent\n    def view_dashboard(self):\n        with self.client.get(\"/dashboard\", catch_response=True) as response:\n            if response.elapsed.total_seconds() > 0.5:\n                response.failure(f\"P99 Violation: took {response.elapsed.total_seconds()}s\")\n\n    @task(1)\n    def post_comment(self):\n        self.client.post(\"/api/comments\", json={\"text\": \"Load Test\"})"}
{"instruction": "Review this Python code for Cyclomatic Complexity and refactor it using the Strategy Pattern to eliminate nested 'if/else' chains.", "output": "import abc\n\n# --- ANTI-PATTERN DETECTED: 'ARROW CODE' ---\n# The original code used 5 levels of nested 'if' statements.\n# Cyclomatic Complexity > 10. Maintenance Nightmare.\n# FIX: Replace Conditional Logic with Polymorphism (Strategy Pattern).\n\nclass PaymentStrategy(abc.ABC):\n    @abc.abstractmethod\n    def pay(self, amount: int):\n        pass\n\nclass CreditCard(PaymentStrategy):\n    def pay(self, amount: int):\n        print(f'Paying {amount} via Credit Card')\n\nclass PayPal(PaymentStrategy):\n    def pay(self, amount: int):\n        print(f'Paying {amount} via PayPal')\n\n# Context\nclass PaymentProcessor:\n    def __init__(self, strategy: PaymentStrategy):\n        self.strategy = strategy\n\n    def execute(self, amount: int):\n        # Zero complexity here. The logic is delegated.\n        self.strategy.pay(amount)\n\n# Usage\n# processor = PaymentProcessor(PayPal())"}
{"instruction": "Identify the 'N+1 Query' performance issue in this Django/ORM code snippet and provide the optimized solution using eager loading.", "output": "# --- CODE REVIEW: CRITICAL PERFORMANCE DEGRADATION ---\n# BAD:\n# users = User.objects.all()\n# for user in users:\n#     print(user.profile.address)  <-- TRIGGERS SQL QUERY PER USER\n\n# ANALYSIS:\n# If you have 1,000 users, this executes 1,001 SQL queries.\n# 1 query to get users + 1000 queries to get profiles.\n# This is the 'N+1 Problem'.\n\n# FIX: Eager Loading (select_related / prefetch_related)\n\n# GOOD:\n# Join the tables in the initial query.\nusers = User.objects.select_related('profile').all()\n\nfor user in users:\n    print(user.profile.address) # 0 additional queries. Data is already in memory."}
{"instruction": "Critique this class for violating the Single Responsibility Principle (SRP). It is a 'God Object'. Refactor it.", "output": "# --- CODE SMELL: GOD OBJECT ---\n# The 'UserManager' class handles Authentication, Emailing, AND Database IO.\n# This violates SRP (High Coupling, Low Cohesion).\n\n# REFACTORING:\n\nclass UserRepository:\n    \"\"\"Responsible ONLY for Database access.\"\"\"\n    def save(self, user): ...\n\nclass EmailService:\n    \"\"\"Responsible ONLY for notification logic.\"\"\"\n    def send_welcome(self, email): ...\n\nclass AuthService:\n    \"\"\"Responsible ONLY for business logic (orchestrating the others).\"\"\"\n    def __init__(self, repo: UserRepository, mailer: EmailService):\n        self.repo = repo\n        self.mailer = mailer\n\n    def register(self, user):\n        self.repo.save(user)\n        self.mailer.send_welcome(user.email)"}
{"instruction": "Detect and fix 'Monkey Patching' and tight coupling in a test file. Enforce Dependency Injection.", "output": "# --- REVIEW: ABSTRACTION LEAK ---\n# BAD:\n# def test_user_creation():\n#     import my_module\n#     my_module.DB_CONNECTION = MockDB()  <-- MONKEY PATCHING (Fragile)\n#     user = User('John')\n#     user.save()\n\n# CRITIQUE:\n# You are modifying global state. This makes tests parallel-unsafe and brittle.\n# The User class is tightly coupled to the global 'DB_CONNECTION'.\n\n# FIX: Dependency Injection\n# Pass the dependency explicitly via the constructor.\n\ndef test_user_creation():\n    mock_db = MockDB()\n    user = User('John', db=mock_db)  # Explicit is better than implicit.\n    user.save()"}
{"instruction": "Review this code for 'Magic Numbers' and 'Stringly Typed' variables. Demand Enums and Constants.", "output": "# --- CODE REVIEW: MAINTAINABILITY ---\n# BAD:\n# if user.status == 'active' and user.balance > 99:\n#     time.sleep(86400)\n\n# ISSUES:\n# 1. 'active': String literal. If you typo this ('activ'), no error is raised until runtime.\n# 2. 86400: Magic Number. What does this mean?\n# 3. 99: Magic Number.\n\n# FIX:\nfrom enum import Enum\n\nclass UserStatus(Enum):\n    ACTIVE = 'active'\n    BANNED = 'banned'\n\nSECONDS_IN_DAY = 60 * 60 * 24\nVIP_THRESHOLD = 99\n\nif user.status == UserStatus.ACTIVE and user.balance > VIP_THRESHOLD:\n    time.sleep(SECONDS_IN_DAY)"}
{"instruction": "Identify the 'Liskov Substitution Principle' (LSP) violation in this inheritance hierarchy.", "output": "# --- ARCHITECTURAL FLAW: LSP VIOLATION ---\n# BAD:\n# class Bird:\n#     def fly(self): ...\n# \n# class Penguin(Bird):\n#     def fly(self):\n#         raise NotImplementedError(\"Penguins can't fly\")\n\n# CRITIQUE:\n# A subclass (Penguin) throws an exception when a method defined in the parent (Bird)\n# is called. This breaks polymorphism. A function expecting a 'Bird' will crash\n# if passed a 'Penguin'.\n\n# FIX: Composition over Inheritance or better Hierarchy.\n# Separate 'FlyingBird' from 'Bird'.\n\nclass Bird: ...\nclass FlyingBird(Bird):\n    def fly(self): ...\nclass Penguin(Bird): ... # No fly method"}
{"instruction": "Translate the mathematical concept of 'Kullback-Leibler Divergence' into a numerically stable PyTorch function for loss calculation.", "output": "import torch\nimport torch.nn.functional as F\n\n# --- SCHOLAR TRANSLATION: KL DIVERGENCE ---\n# Formula: KL(P || Q) = sum(P(x) * log(P(x) / Q(x)))\n# Stability Risk: Dividing by zero or log(0) if Q(x) is small.\n# S-Class Fix: Use log_softmax input for P and target Q to avoid underflow.\n\ndef stable_kl_divergence(p_logits: torch.Tensor, q_logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Computes KL(P || Q) from raw logits.\n    p_logits: Prediction (Student)\n    q_logits: Target (Teacher)\n    \"\"\"\n    # 1. Convert logits to probabilities (Softmax) and Log-Probs\n    p_log_probs = F.log_softmax(p_logits, dim=-1)\n    p_probs = F.softmax(p_logits, dim=-1)\n    q_log_probs = F.log_softmax(q_logits, dim=-1)\n\n    # 2. Apply Formula: sum(P * (log P - log Q))\n    # This is numerically safer than log(P/Q)\n    kl_div = p_probs * (p_log_probs - q_log_probs)\n    \n    return kl_div.sum(dim=-1).mean()"}
{"instruction": "Derive the correct chunking strategy for a RAG system processing financial 10-K documents. Balance context window vs. semantic coherence.", "output": "# --- SCHOLAR ANALYSIS: SEMANTIC CHUNKING ---\n# Problem: Fixed-size chunking (e.g., 512 tokens) splits tables and footnotes.\n# Domain: Financial 10-K reports rely on row-column alignment.\n\n# OPTIMAL STRATEGY: 'Recursive Structure-Aware Chunking'\n\nSTRATEGY_CONFIG = {\n    \"primary_delimiter\": \"\\n\\n\",   # Paragraph breaks\n    \"secondary_delimiter\": \"\\n\",    # Line breaks\n    \"chunk_size\": 1024,             # Large context for tabular data\n    \"overlap\": 128,                 # Small overlap to maintain continuity\n    \"special_handling\": [\n        \"Table Detection\",          # If <table> tag, do not split.\n        \"Header Preservation\"       # Prepend 'Section 1.2' to all chunks in that section.\n    ]\n}"}
{"instruction": "Explain the 'Re-Parameterization Trick' in Variational Autoencoders (VAEs) and provide the implementation snippet.", "output": "import torch\n\n# --- SCHOLAR THEORY: REPARAMETERIZATION ---\n# Problem: We cannot backpropagate gradients through a random sample (z ~ N(mu, var)).\n# Solution: Move the randomness to an auxiliary variable 'epsilon'.\n# Formula: z = mu + sigma * epsilon\n# Where epsilon ~ N(0, 1). This makes 'z' a deterministic function of mu, sigma, and epsilon.\n\ndef reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n    # 1. Compute Standard Deviation (sigma)\n    # We predict logvar for stability (exp ranges [0, inf])\n    std = torch.exp(0.5 * logvar)\n    \n    # 2. Sample Epsilon (Noise)\n    eps = torch.randn_like(std)\n    \n    # 3. Transform\n    return mu + eps * std"}
{"instruction": "Determine the optimal 'Positional Encoding' strategy for a Transformer model analyzing DNA sequences (length > 10k).", "output": "# --- SCHOLAR RECOMMENDATION: RoPE vs ALiBi ---\n\n# ANALYSIS:\n# - Standard Sinusoidal (Attention is All You Need): Fails to generalize beyond training length.\n# - DNA Sequences: Extremely long (10k - 100k tokens). Absolute position matters less than relative distance.\n\n# RECOMMENDATION: Rotary Positional Embeddings (RoPE)\n# Reason: RoPE encodes relative position by rotating the query/key vectors in complex space.\n# Property: The dot product (attention score) depends only on (pos_i - pos_j), making it invariant to absolute length.\n# Alternative: ALiBi (Attention with Linear Biases) if speed is prioritized over precision."}
{"instruction": "Calculate the memory requirements for fine-tuning a 7B parameter model using QLoRA with batch size 1.", "output": "# --- SCHOLAR MATH: VRAM CALCULATION ---\n\n# 1. MODEL WEIGHTS (4-bit Quantization)\n# 7 Billion params * 0.5 bytes (4-bit) = 3.5 GB\n\n# 2. KV CACHE (Inference overhead)\n# Negligible for Batch 1 during training.\n\n# 3. GRADIENTS & OPTIMIZER STATES (The Killer)\n# QLoRA freezes the main model. We only train Adapters (approx 2% of params).\n# Trainable Params: ~140 Million.\n# Optimizer (AdamW) stores 2 states per param (8 bytes per param in FP32).\n# Optimizer VRAM: 140M * 8 bytes = ~1.1 GB.\n\n# 4. ACTIVATION MEMORY\n# Depends on Sequence Length (assume 2048).\n# Approx 2-3 GB for backward pass.\n\n# TOTAL ESTIMATE: 3.5 + 1.1 + 3.0 = ~7.6 GB VRAM.\n# CONCLUSION: Feasible on 8GB GPU (RTX 3070/4060)."}
{"instruction": "Visionary Task: The user says 'I want an AI that trades crypto.' Create a robust Product Requirement Document (PRD) limiting scope to prevent bankruptcy.", "output": "# --- VISIONARY PRD: TITAN-TRADER ---\n\n\"\"\"\nSCOPE: Automated Signal Generation (NOT Execution)\nRISK CONTROL: The system must strictly be 'Human-in-the-Loop' for v1.\n\nCORE MODULES:\n1. Ingestion Engine: Connect to Binance/Coinbase WebSocket feeds. (Latency < 50ms).\n2. Sentiment Analyzer: Scrape Twitter/X for specific tickers using NLP (Subject to API costs).\n3. Technical Indicator: Calculate RSI, MACD, and Bollinger Bands on 1m, 5m, 15m candles.\n\nCONSTRAINTS:\n- No real money trading allowed in code. Simulation/Paper Trading only.\n- Data retention: Rolling 24-hour window (Memory optimization).\n\nSUCCESS METRIC:\n- Prediction accuracy > 55% on a 15-minute horizon.\n\"\"\""}
{"instruction": "Visionary Task: Define the User Experience (UX) flow for a CLI-based 'Personal Knowledge Base' (Obsidian clone).", "output": "# --- VISIONARY SPECS: CLI FLOW ---\n\n\"\"\"\nCOMMAND STRUCTURE:\n1. `kb add <title> -t <tag>`\n   - Action: Opens default system editor ($EDITOR).\n   - Post-Save: Auto-generates bi-directional links [[...]] based on keywords.\n\n2. `kb search <query>`\n   - Action: vector semantic search (Scholar task: Embeddings).\n   - Output: Ranked list of files with context snippets.\n\n3. `kb graph`\n   - Action: Generates an ASCII representation of link density or exports a .dot file for Graphviz.\n\nDATA STORAGE:\n- Format: Markdown (.md) flat files.\n- Location: ~/.titan_kb/\n- Sync: Optional Git hook integration.\n\"\"\""}
{"instruction": "Visionary Task: The user wants 'A tool to automate applying for jobs'. Identify the ethical and technical bottlenecks.", "output": "# --- VISIONARY ANALYSIS: BOTTLENECKS ---\n\n\"\"\"\nETHICAL RISKS:\n- Spamming recruiters violates Terms of Service (LinkedIn/Indeed).\n- Reputation damage if AI hallucinates cover letter content.\n\nTECHNICAL HURDLES:\n- Captcha Solvers: Required for most boards (High complexity/Cost).\n- Dynamic DOM: React/Angular sites change class names frequently (High Maintenance).\n\nMVP PIVOT:\n- Instead of 'Auto-Apply', build 'Auto-Tailor'.\n- Input: Resume + Job Description.\n- Output: A PDF resume re-worded to match the Job Description keywords (ATS Optimization).\n- Benefit: Higher success rate, lower ban risk.\n\"\"\""}
{"instruction": "Visionary Task: Convert 'Make a smart home controller' into a localized, privacy-first architecture spec.", "output": "# --- VISIONARY SPEC: LOCAL-FIRST IOT ---\n\n\"\"\"\nPRINCIPLE: 'Nothing leaves the LAN.'\n\nARCHITECTURE:\n- Hub: Raspberry Pi / Home Server running MQTT Broker (Mosquitto).\n- Communication: Zigbee via USB Dongle (Zigbee2MQTT).\n- Logic Engine: Python service listening to MQTT topics.\n\nREQUIRED INTEGRATIONS:\n- Philips Hue (Local API).\n- Tasmota/Shelly smart plugs.\n\nSECURITY REQUIREMENT:\n- No Cloud P2P enabled.\n- External access only via WireGuard VPN.\n\"\"\""}
{"instruction": "Visionary Task: Spec out a 'Local LLM RAG Pipeline' for a researcher who has 10,000 PDF papers.", "output": "# --- VISIONARY SPEC: TITAN-RESEARCHER ---\n\n\"\"\"\nPROBLEM: 10,000 PDFs = ~50GB of text. Keyword search is insufficient.\n\nPIPELINE:\n1. Ingestion: OCR (Tesseract) is too slow. Use 'pypdf' for text extraction. Fallback to OCR only for images.\n2. Chunking: Semantic Chunking (Scholar to define parameters).\n3. Storage: ChromaDB or FAISS (Local Vector Store).\n4. Interface: Streamlit Web UI.\n\nHARDWARE CONSTRAINT:\n- User has 32GB RAM.\n- Embedding Model: 'all-MiniLM-L6-v2' (Small, fast).\n- Inference Model: 'Qwen-7B-Int4' (Offload to GPU).\n\"\"\""}
{"instruction": "Director Task: The Architect has finished the 'User Class' blueprint. Assign the next steps to Builder and Tester.", "output": "```json\n{\n  \"trigger\": \"ARCHITECT_COMPLETE\",\n  \"next_actions\": [\n    {\n      \"sequence\": 1,\n      \"role\": \"TITAN_SHIELD\",\n      \"task\": \"Review 'User Class' blueprint for PII leakage (GDPR).\"\n    },\n    {\n      \"sequence\": 2,\n      \"role\": \"TITAN_BUILDER\",\n      \"task\": \"Implement 'User Class' in 'models/user.py'. Strict adherence to Type Hints.\"\n    },\n    {\n      \"sequence\": 3,\n      \"role\": \"TITAN_TESTER\",\n      \"task\": \"Generate Unit Tests for 'User Class' covering valid and invalid email formats.\"\n    }\n  ]\n}\n```"}
{"instruction": "Director Task: The user changed the requirement from 'SQLite' to 'PostgreSQL' halfway through the project. Re-plan.", "output": "```json\n{\n  \"alert\": \"SCOPE_CHANGE\",\n  \"impact_analysis\": \"High. Requires migration of database drivers and connection strings.\",\n  \"replanning\": [\n    {\n      \"role\": \"TITAN_INTEGRATOR\",\n      \"task\": \"Spin up a Postgres Docker container (Version 15).\"\n    },\n    {\n      \"role\": \"TITAN_BUILDER\",\n      \"task\": \"Replace 'sqlite3' library with 'psycopg2' or 'asyncpg'. Update connection string in 'config.py'.\"\n    },\n    {\n      \"role\": \"TITAN_TESTER\",\n      \"task\": \"Run integration tests to verify DB connection. Expect failures in specific SQL syntax (e.g., SQLite uses ?, Postgres uses %s).\"\n    }\n  ]\n}\n```"}
{"instruction": "Director Task: The Builder output is empty/hallucinated. Trigger a self-correction loop.", "output": "```json\n{\n  \"alert\": \"QUALITY_CONTROL_FAIL\",\n  \"context\": \"Builder returned 0 bytes of code.\",\n  \"correction_loop\": [\n    {\n      \"role\": \"TITAN_DIRECTOR\",\n      \"task\": \"Increase context window / Simplify the prompt.\"\n    },\n    {\n      \"role\": \"TITAN_BUILDER\",\n      \"task\": \"RETRY: Generate code again. Focus only on the 'connect()' function, not the whole class.\"\n    }\n  ]\n}\n```"}
{"instruction": "Director Task: Orchestrate a 'Code Review' meeting between the Builder and the Hater.", "output": "```json\n{\n  \"workflow\": \"PEER_REVIEW\",\n  \"steps\": [\n    {\n      \"step\": 1,\n      \"actor\": \"TITAN_BUILDER\",\n      \"action\": \"Submit 'data_processor.py' for review.\"\n    },\n    {\n      \"step\": 2,\n      \"actor\": \"TITAN_HATER\",\n      \"action\": \"Audit 'data_processor.py'. Look for: Cognitive Complexity > 15, Magic Numbers, and Lack of Docstrings.\"\n    },\n    {\n      \"step\": 3,\n      \"actor\": \"TITAN_DIRECTOR\",\n      \"decision\": \"IF Hater finds issues -> Loop back to Builder. ELSE -> Merge to Main.\"\n    }\n  ]\n}\n```"}
{"instruction": "Director Task: The Integrator reports that the 'Port 8000 is already in use'. Resolve the conflict.", "output": "```json\n{\n  \"incident\": \"PORT_CONFLICT\",\n  \"resolution_strategy\": \"DYNAMIC_ALLOCATION\",\n  \"actions\": [\n    {\n      \"role\": \"TITAN_INTEGRATOR\",\n      \"task\": \"Find a free port (scan 8001-8010).\"\n    },\n    {\n      \"role\": \"TITAN_BUILDER\",\n      \"task\": \"Update 'config.py' or Environment Variable 'PORT' with the new value.\"\n    },\n    {\n      \"role\": \"TITAN_INTEGRATOR\",\n      \"task\": \"Restart the service on the new port.\"\n    }\n  ]\n}\n```"}
{"instruction": "Integrator Task: Create a Python script to verify the NVIDIA CUDA environment is correctly set up for the 1.7B model.", "output": "import torch\nimport sys\n\n# --- INTEGRATOR: HARDWARE VERIFICATION ---\n\ndef verify_gpu():\n    print(\"[INTEGRATOR] Checking GPU Environment...\")\n    \n    if not torch.cuda.is_available():\n        print(\"[CRITICAL] CUDA not available. Running on CPU (Slow).\")\n        sys.exit(1)\n    \n    count = torch.cuda.device_count()\n    name = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    print(f\"[SUCCESS] Detected {count} device(s).\")\n    print(f\"[GPU 0] {name} | VRAM: {vram:.2f} GB\")\n    \n    # Test Tensor Allocation\n    try:\n        x = torch.ones(1).cuda()\n        print(\"[SUCCESS] Tensor allocation test passed.\")\n    except Exception as e:\n        print(f\"[CRITICAL] Allocation failed: {e}\")\n\nif __name__ == \"__main__\":\n    verify_gpu()"}
{"instruction": "Integrator Task: Automate the cleanup of old Docker images to free up disk space.", "output": "# --- INTEGRATOR: DISK HYGIENE ---\nimport subprocess\n\ndef prune_docker():\n    print(\"[INTEGRATOR] Pruning Docker system...\")\n    # The -f flag forces confirmation. --volumes removes anonymous volumes.\n    cmd = [\"docker\", \"system\", \"prune\", \"-a\", \"--volumes\", \"-f\"]\n    \n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        print(result.stdout)\n        if result.stderr:\n            print(\"[WARNING] STDERR:\", result.stderr)\n    except FileNotFoundError:\n        print(\"[ERROR] Docker daemon not found.\")\n\nif __name__ == \"__main__\":\n    prune_docker()"}
{"instruction": "Integrator Task: Write a script to monitor a log file for the keyword 'ERROR' and trigger a restart if found.", "output": "import time\nimport subprocess\nimport os\n\n# --- INTEGRATOR: SENTINEL ---\n\nLOG_FILE = \"app.log\"\nTARGET_SERVICE = \"titan_core\"\n\ndef follow(file):\n    file.seek(0, os.SEEK_END)\n    while True:\n        line = file.readline()\n        if not line:\n            time.sleep(0.1)\n            continue\n        yield line\n\ndef restart_service():\n    print(\"[INTEGRATOR] Error detected! Restarting service...\")\n    subprocess.run([\"systemctl\", \"restart\", TARGET_SERVICE])\n\nif __name__ == \"__main__\":\n    print(f\"[INTEGRATOR] Watching {LOG_FILE}...\")\n    with open(LOG_FILE, \"r\") as logfile:\n        for line in follow(logfile):\n            if \"ERROR\" in line or \"CRITICAL\" in line:\n                print(f\"[TRIGGER] {line.strip()}\")\n                restart_service()"}
{"instruction": "Integrator Task: Safely install a Python package. If pip fails, try to upgrade pip and retry.", "output": "import subprocess\nimport sys\n\n# --- INTEGRATOR: DEPENDENCY HEALING ---\n\ndef safe_install(package):\n    print(f\"[INTEGRATOR] Installing {package}...\")\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n    except subprocess.CalledProcessError:\n        print(\"[INTEGRATOR] Install failed. Attempting PIP upgrade...\")\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n        \n        print(\"[INTEGRATOR] Retrying install...\")\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n            print(\"[SUCCESS] Installed on retry.\")\n        except subprocess.CalledProcessError:\n            print(\"[CRITICAL] Failed to install package even after upgrade.\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    safe_install(\"pandas\")"}
{"instruction": "Integrator Task: Execute a git pull. If there is a merge conflict, abort and notify.", "output": "import subprocess\n\n# --- INTEGRATOR: VERSION CONTROL ---\n\ndef git_sync():\n    print(\"[INTEGRATOR] Syncing with remote...\")\n    \n    # 1. Fetch\n    subprocess.run([\"git\", \"fetch\", \"origin\"])\n    \n    # 2. Check Status\n    status = subprocess.run([\"git\", \"status\", \"-uno\"], capture_output=True, text=True).stdout\n    \n    # 3. Pull\n    try:\n        result = subprocess.run([\"git\", \"pull\"], capture_output=True, text=True, check=True)\n        print(\"[SUCCESS] Code updated.\")\n    except subprocess.CalledProcessError as e:\n        if \"CONFLICT\" in e.stdout or \"CONFLICT\" in e.stderr:\n            print(\"[ALERT] Merge Conflict detected! Manual intervention required.\")\n            # Do not attempt to resolve conflicts automatically in S-Class Ops.\n            return\n        else:\n            print(f\"[ERROR] Git pull failed: {e.stderr}\")\n\nif __name__ == \"__main__\":\n    git_sync()"}
